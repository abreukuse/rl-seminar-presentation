---
marp: true
theme: gaia
paginate: true
html: true
---

# RL Multi-Objetivo Aplicado ao Problema Integrado de Patrulha e Despacho

Moacir Almeida Sim√µes J√∫nior
Tobias de Abreu Kuse



---

### O Problema Central: Um Conflito de Objetivos

- Opera√ß√µes policiais t√™m dois objetivos conflitantes e simult√¢neos.  
- **1. Patrulhamento Proativo:** Maximizar a *presen√ßa* em √°reas de alto risco ("hotspots") para dissuadir o crime.  
- **2. Despacho Reativo:** Minimizar o *tempo de resposta* a novas chamadas de emerg√™ncia din√¢micas.  
- **O Desafio:** Cada vez que uma unidade √© despachada *reativamente*, ela compromete o plano de patrulha *proativo*.  
- Este √© um problema **bi-objetivo**.

---

### Um Panorama das Solu√ß√µes

- Problema complexo: mistura previs√£o espa√ßo-temporal, roteamento e eventos estoc√°sticos em tempo real.  
- Tr√™s filosofias de solu√ß√£o principais:  
  1. Modelos **H√≠bridos Heur√≠sticos** (Simula√ß√£o + Meta-heur√≠sticas)  
  2. Modelos **MARL Descentralizados** (RL Multiagente)  
  3. Modelos **MORL Centralizados** (RL Multi-Objetivo)

---

### Abordagem 1: H√≠brida-Heur√≠stica  
*(Sim√µes J√∫nior & Borenstein, 2025)*

- **Conceito:** Modelar o sistema com simula√ß√£o de eventos discretos.  
- **M√©todo:**  
  - **Prever:** Usar ML (ex: XGBoost) para prever *hotspots din√¢micos*.  
  - **Patrulhar:** Usar **Otimiza√ß√£o por Col√¥nia de Formigas (ACO)** para rotas de patrulha ideais.  
  - **Testar:** Simular interrup√ß√µes por chamadas aleat√≥rias.  
- **Conclus√£o:** Abordagem de *"otimizar e depois simular"*.

---

### Abordagem 1: M√©todo e Hotspots Din√¢micos

<style>
img[alt~="left"] {
  position: absolute;
  top: 20%;     /* Dist√¢ncia do topo */
  left: 2%;      /* Alinha na esquerda */
  width: 47%;    /* Largura da imagem */
  height: 70%;   /* Altura m√°xima */
  object-fit: contain; /* Previne distor√ß√£o/corte */
}
img[alt~="right"] {
  position: absolute;
  top: 20%;     /* Mesma dist√¢ncia do topo */
  right: 2%;     /* Alinha na direita */
  width: 47%;    /* Largura da imagem */
  height: 70%;   /* Altura m√°xima */
  object-fit: contain; /* Previne distor√ß√£o/corte */
}
</style>

![left](images/fig1.png)
![right](images/fig2.png)

---

<!-- ### Abordagem 1: Fluxograma do M√©todo

<style>
img[alt~="center"] {
  position: absolute;
  top: 55%;  /* Ponto central vertical (55% para dar espa√ßo ao t√≠tulo) */
  left: 50%; /* Ponto central horizontal */
  transform: translate(-50%, -50%); /* Puxa a imagem de volta pelo seu pr√≥prio centro */
  
  /* Limites para evitar corte */
  max-height: 70%; 
  max-width: 90%;
  
  object-fit: contain; /* Mant√©m a propor√ß√£o sem distorcer */
}
</style>


--- -->

### Abordagem 2: Modelo Multiagente  
*(Repasky et al., 2024)*

- **Conceito:** Cada viatura = um agente de RL.  
- **M√©todo:**  
  - Sistema **heterog√™neo** com $N+1$ agentes.  
  - **$N$ Patrulheiros:** aprendem pol√≠ticas pr√≥prias (DQN compartilhada).  
  - **1 Despachante:** aprende pol√≠tica central (MIP + VFA).  
- **Conclus√£o:** Abordagem **descentralizada**, com agentes aprendendo individualmente.

---

### Abordagem 2: Visualiza√ß√£o do Modelo MARL

<style>
img[alt~="left"] {
  position: absolute;
  top: 20%;     /* Dist√¢ncia do topo */
  left: 2%;      /* Alinha na esquerda */
  width: 47%;    /* Largura da imagem */
  height: 70%;   /* Altura m√°xima */
  object-fit: contain; /* Previne distor√ß√£o/corte */
}
img[alt~="right"] {
  position: absolute;
  top: 20%;     /* Mesma dist√¢ncia do topo */
  right: 2%;     /* Alinha na direita */
  width: 47%;    /* Largura da imagem */
  height: 70%;   /* Altura m√°xima */
  object-fit: contain; /* Previne distor√ß√£o/corte */
}
</style>

![left](images/fig3.png)
![right](images/fig9.png)

---

### Abordagem 3: RL Multi-Objetivo (MORL)

- RL padr√£o ‚Üí recompensa escalar $R$.  
- MORL ‚Üí vetor de recompensas $\vec{R} = \langle R_{\text{resposta}}, R_{\text{presen√ßa}} \rangle$.  
- **Desafio:** "√ìtimo" √© subjetivo (quanto vale piorar patrulha pra melhorar resposta?).  
- **Solu√ß√£o comum:** Soma ponderada  
  $$R_{total} = w_1 R_{resposta} + w_2 R_{presen√ßa}$$  
- **Problema:** Altamente sens√≠vel aos pesos ‚Äî arbitr√°rios e dif√≠ceis de justificar.

---

### Vis√£o Geral e Formula√ß√£o

- Para contornar as dificuldades da abordagem de soma ponderada, o artigo de **Joe, Lau, & Pan (2022)** apresenta um modelo MORL centralizado que n√£o utiliza essa t√©cnica.
- **T√≠tulo:** *Reinforcement Learning Approach to Solve Dynamic Bi-objective Police Patrol Dispatching and Rescheduling Problem*  
- **Formula√ß√£o:** MDP Centralizado de Agente √önico.  
- **Agente de RL:** Planejador central / despachante.  
- **Recursos:** Unidades de patrulha (recursos controlados).  
- N√£o √© MARL ‚Äî unidades n√£o aprendem.

---

### O MDP: Estado e A√ß√£o

- **Estado ($S_k$):** √â a tupla: $\langle t_k, \delta(k), \sigma(k), \omega_k \rangle$
  - $t_k$: Hora atual
  - $\delta(k)$: Cronogramas conjuntos atuais
  - $\sigma(k)$: Status da patrulha
  - $\omega_k$: O novo incidente (local, tempo, dura√ß√£o)

- **A√ß√£o ($x_k$):** tupla de decis√£o $\langle x_k^i, x_k^t, \delta^x(k) \rangle$.
  - $x_k^i$: quem despachar
  - $x_k^t$: quando
  - $\delta^x(k)$: novo cronograma unificado

---

### O MDP: Exemplo de Cronograma Conjunto ($\delta(k)$)

<style>
img[alt~="center"] {
  position: absolute;
  top: 55%;  /* Ponto central vertical (55% para dar espa√ßo ao t√≠tulo) */
  left: 50%; /* Ponto central horizontal */
  transform: translate(-50%, -50%); /* Puxa a imagem de volta pelo seu pr√≥prio centro */
  
  /* Limites para evitar corte */
  max-height: 70%; 
  max-width: 90%;
  
  object-fit: contain; /* Mant√©m a propor√ß√£o sem distorcer */
}
</style>

![center](images/fig7.png)

---

### MDP: Transi√ß√£o e Recompensa

- **Transi√ß√£o ($S_k \rightarrow S_k^x$):**
  - A transi√ß√£o do estado √© determin√≠stica.
  - A A√ß√£o $x_k$ (o novo cronograma $\delta^x(k)$) √© aplicada.
  - O estado muda de "pr√©-decis√£o" ($S_k$) para "p√≥s-decis√£o" ($S_k^x$).

- **Recompensa ($R$): multiplicativa e diferencial**
    $$R(S_k, x_k) = f_r(x_k) \times (f_p(\delta^x(k)) - f_p(\delta(k)))$$
  - **Componentes:**
    - $f_r(x_k) \rightarrow$ Sucesso da Resposta (1.0, 0.5, 0)
    - $f_p(\delta^x(k)) - f_p(\delta(k)) \rightarrow$ Mudan√ßa na Qualidade da Patrulha
---

### MDP: A Fun√ß√£o Objetivo

- O objetivo do agente √© encontrar a a√ß√£o $x_k^*$ que resolve a Equa√ß√£o de Otimiza√ß√£o:
  $$x_k^* = \text{argmax}_{x_k} \{ R(S_k, x_k) + \gamma\hat{V}(S_k^x) \}$$

- **O Desafio Computacional:**
  - Como resolver este `argmax`?
  - O espa√ßo de a√ß√µes $x_k$ (os novos cronogramas) √© de alta dimensionalidade (combinatorial).
  - A solu√ß√£o padr√£o (ex: DQN, que itera sobre todas as a√ß√µes) n√£o √© aplic√°vel.

---

### A Solu√ß√£o: Arquitetura H√≠brida

- O artigo resolve o `argmax` intrat√°vel com uma **divis√£o de trabalho**:

- **1. Gerador de A√ß√µes (Online):**
  - Uma heur√≠stica cl√°ssica (Ejection Chains) gera um conjunto pequeno de a√ß√µes $x_k$ (novos cronogramas) que s√£o vi√°veis e de alta qualidade.

- **2. Avaliador de A√ß√µes (Offline):**
  - Uma Rede de Fun√ß√£o Valor ($\hat{V}$) √© treinada offline com Dados Hist√≥ricos (Experience Replay).
  - O √∫nico trabalho desta rede √© calcular o valor futuro $\hat{V}(S_k^x)$ (a "nota") para qualquer cronograma.

- **A Decis√£o (Eq. 6):**
  - O sistema aplica a heur√≠stica para gerar as op√ß√µes $x_k$.
  - O $\hat{V}$ treinado avalia (d√° a nota) para cada op√ß√£o.
  - O agente simplesmente escolhe a op√ß√£o com a maior nota (Recompensa Imediata + Valor Futuro).

---

### A Solu√ß√£o: Arquitetura do Modelo

<style>
img[alt~="center"] {
  position: absolute;
  top: 55%;  /* Ponto central vertical (55% para dar espa√ßo ao t√≠tulo) */
  left: 50%; /* Ponto central horizontal */
  transform: translate(-50%, -50%); /* Puxa a imagem de volta pelo seu pr√≥prio centro */
  
  /* Limites para evitar corte */
  max-height: 70%; 
  max-width: 90%;
  
  object-fit: contain; /* Mant√©m a propor√ß√£o sem distorcer */
}
</style>

![center](images/fig5.png)

---

### Inova√ß√£o Bi-Objetivo (Sem Soma Ponderada)

- Recompensa **multiplicativa e diferencial**:
  $$R(S_k, x_k) = f_r(x_k) \times f_p(\delta^x(k)) - f_p(\delta(k))$$
- **Componentes:**
  - $f_r(x_k)$ ‚Üí sucesso da resposta (1.0, 0.5, 0)  
  - $f_p(\delta^x(k)) - f_p(\delta(k))$ ‚Üí mudan√ßa na presen√ßa da patrulha  
- **Justificativa da Abordagem:**  
  - A recompensa pondera *resposta* √ó *impacto na patrulha*.  
  - Uma falha que destr√≥i o plano de patrulha ‚Üí fortemente penalizada.

---

### Resultados Chave

- **Desempenho do Aprendizado:**
  - O modelo "integrado" (Joint) aprende mais r√°pido e atinge uma recompensa cumulativa maior e mais est√°vel.
  - O m√©todo "Two-Stage" se mostra inst√°vel e inferior, especialmente em cen√°rios complexos.

- **Taxa de Sucesso Final:**
  - O h√≠brido ($\hat{V}$ + heur√≠stica) supera estatisticamente o m√©todo "Two-Stage" na taxa de sucesso de resposta.

<!-- ---

### Resultados: An√°lise Gr√°fica

<style>
img[alt~="left"] {
  position: absolute; top: 20%; left: 2%;
  width: 47%; height: 70%; object-fit: contain;
}
img[alt~="right"] {
  position: absolute; top: 20%; right: 2%;
  width: 47%; height: 70%; object-fit: contain;
}
</style>

![left](images/fig8.png)

![right](images/fig6.png) -->

<!-- ---

### Resumo das Abordagens

- **Problema:** Patrulha Integrada (Proativa) & Despacho (Reativo).  
- **H√≠brida/Heur√≠stica:** poderosa, mas depende de heur√≠sticas (ACO).  
- **MARL:** baseado em agentes, mas decomp√µe o problema.  
- **MORL Centralizado:** abordagem hol√≠stica (VFA + heur√≠stica inteligente).

---

### Principais Conclus√µes

- Problema real e ideal para RL avan√ßado.  
- Arquitetura de Joe, Lau & Pan:  
  - **Gerador-Heur√≠stico + Avaliador-VFA** ‚Üí √≥timo para espa√ßos de a√ß√£o complexos.  
- Recompensa multiplicativa ‚Üí resolve bi-objetivo sem pesos arbitr√°rios. -->

---

# Obrigado! üôå  
Perguntas?
