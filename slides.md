---
marp: true
theme: gaia
paginate: true
html: true
---

# RL Multi-Objetivo Aplicado ao Problema Integrado de Patrulha e Despacho

Moacir Almeida Sim√µes J√∫nior
Tobias de Abreu Kuse



---

### O Problema Central: Um Conflito de Objetivos

- Opera√ß√µes policiais t√™m dois objetivos conflitantes e simult√¢neos.  
- **1. Patrulhamento Proativo:** Maximizar a *presen√ßa* em √°reas de alto risco ("hotspots") para dissuadir o crime.  
- **2. Despacho Reativo:** Minimizar o *tempo de resposta* a novas chamadas de emerg√™ncia din√¢micas.  
- **O Desafio:** Cada vez que uma unidade √© despachada *reativamente*, ela compromete o plano de patrulha *proativo*.  
- Este √© um problema **bi-objetivo**.

---

### Um Panorama das Solu√ß√µes

- Problema complexo: mistura previs√£o espa√ßo-temporal, roteamento e eventos estoc√°sticos em tempo real.  
- Tr√™s filosofias de solu√ß√£o principais:  
  1. Modelos **H√≠bridos Heur√≠sticos** (Simula√ß√£o + Meta-heur√≠sticas)  
  2. Modelos **MARL Descentralizados** (RL Multiagente)  
  3. Modelos **MORL Centralizados** (RL Multi-Objetivo)

---

### Abordagem 1: H√≠brida-Heur√≠stica  
*(Sim√µes J√∫nior & Borenstein, 2025)*

- **Conceito:** Modelar o sistema com simula√ß√£o de eventos discretos.  
- **M√©todo:**  
  - **Prever:** Usar ML (ex: XGBoost) para prever *hotspots din√¢micos*.  
  - **Patrulhar:** Usar **Otimiza√ß√£o por Col√¥nia de Formigas (ACO)** para rotas de patrulha ideais.  
  - **Testar:** Simular interrup√ß√µes por chamadas aleat√≥rias.  
- **Conclus√£o:** Abordagem de *"otimizar e depois simular"*.

---

### Abordagem 1: M√©todo e Hotspots Din√¢micos

<style>
img[alt~="left"] {
  position: absolute;
  top: 20%;     /* Dist√¢ncia do topo */
  left: 2%;      /* Alinha na esquerda */
  width: 47%;    /* Largura da imagem */
  height: 70%;   /* Altura m√°xima */
  object-fit: contain; /* Previne distor√ß√£o/corte */
}
img[alt~="right"] {
  position: absolute;
  top: 20%;     /* Mesma dist√¢ncia do topo */
  right: 2%;     /* Alinha na direita */
  width: 47%;    /* Largura da imagem */
  height: 70%;   /* Altura m√°xima */
  object-fit: contain; /* Previne distor√ß√£o/corte */
}
</style>

![left](images/fig1.png)
![right](images/fig2.png)

---

<!-- ### Abordagem 1: Fluxograma do M√©todo

<style>
img[alt~="center"] {
  position: absolute;
  top: 55%;  /* Ponto central vertical (55% para dar espa√ßo ao t√≠tulo) */
  left: 50%; /* Ponto central horizontal */
  transform: translate(-50%, -50%); /* Puxa a imagem de volta pelo seu pr√≥prio centro */
  
  /* Limites para evitar corte */
  max-height: 70%; 
  max-width: 90%;
  
  object-fit: contain; /* Mant√©m a propor√ß√£o sem distorcer */
}
</style>


--- -->

### Abordagem 2: Modelo Multiagente  
*(Repasky et al., 2024)*

- **Conceito:** Cada viatura = um agente de RL.  
- **M√©todo:**  
  - Sistema **heterog√™neo** com $N+1$ agentes.  
  - **$N$ Patrulheiros:** aprendem pol√≠ticas pr√≥prias (DQN compartilhada).  
  - **1 Despachante:** aprende pol√≠tica central (MIP + VFA).  
- **Conclus√£o:** Abordagem **descentralizada**, com agentes aprendendo individualmente.

---

### Abordagem 2: Visualiza√ß√£o do Modelo MARL

<style>
img[alt~="left"] {
  position: absolute;
  top: 20%;     /* Dist√¢ncia do topo */
  left: 2%;      /* Alinha na esquerda */
  width: 47%;    /* Largura da imagem */
  height: 70%;   /* Altura m√°xima */
  object-fit: contain; /* Previne distor√ß√£o/corte */
}
img[alt~="right"] {
  position: absolute;
  top: 20%;     /* Mesma dist√¢ncia do topo */
  right: 2%;     /* Alinha na direita */
  width: 47%;    /* Largura da imagem */
  height: 70%;   /* Altura m√°xima */
  object-fit: contain; /* Previne distor√ß√£o/corte */
}
</style>

![left](images/fig3.png)
![right](images/fig9.png)

---

### Abordagem 3: RL Multi-Objetivo (MORL)

- RL padr√£o ‚Üí recompensa escalar $R$.  
- MORL ‚Üí vetor de recompensas $\vec{R} = \langle R_{\text{resposta}}, R_{\text{presen√ßa}} \rangle$.  
- **Desafio:** "√ìtimo" √© subjetivo (quanto vale piorar patrulha pra melhorar resposta?).  
- **Solu√ß√£o comum:** Soma ponderada  
  $$R_{total} = w_1 R_{resposta} + w_2 R_{presen√ßa}$$  
- **Problema:** Altamente sens√≠vel aos pesos ‚Äî arbitr√°rios e dif√≠ceis de justificar.

---

### MORL: Abordagens Conceituais

<style scoped>
section {
  font-size: 28px;
}
</style>

- **1. M√©todo de Restri√ß√µes (Constraint-based):**
  - Otimiza um objetivo principal e trata os outros como **restri√ß√µes**.
  - **Exemplo:** "Maximizar a *Resposta* (Objetivo 1), contanto que (Restri√ß√£o) a *Presen√ßa da Patrulha* (Objetivo 2) n√£o caia abaixo de 80%."
  - **Vantagem:** Mais intuitivo para um gestor definir um desempenho m√≠nimo do que pesos arbitr√°rios.

- **2. Abordagens Vetoriais (Policy/Value-based):**
  - O agente n√£o aprende um Q-Valor escalar, mas sim um **Q-Vetor**: $\vec{Q}(s, a) = \langle Q_{resposta}, Q_{presen√ßa} \rangle$
  - O objetivo n√£o √© encontrar "um" √≥timo, mas sim o conjunto de todas as **Solu√ß√µes de Trade-off** (n√£o √© poss√≠vel melhorar um objetivo sem piorar o outro).
---

### Estudo de Caso: MORL Aplicado ao Problema Integrado de Patrulhamento e Despacho

<style scoped>
section {
  font-size: 32px;
}
</style>

- Para contornar as dificuldades da "soma ponderada", o artigo de **Joe, Lau, & Pan (2022)** prop√µe uma **fun√ß√£o de recompensa** que n√£o exige atribuir pesos aos objetivos.
- **T√≠tulo:** *Reinforcement Learning Approach to Solve Dynamic Bi-objective Police Patrol Dispatching and Rescheduling Problem*  
- **Formula√ß√£o:** MDP Centralizado de Agente √önico.  
- **Agente de RL:** Planejador central / despachante.  
- **Recursos:** Unidades de patrulha (recursos controlados).  
- N√£o √© MARL ‚Äî unidades n√£o aprendem.

---

### O MDP: Estado e A√ß√£o

- **Estado ($S_k$):** √â a tupla: $\langle t_k, \delta(k), \sigma(k), \omega_k \rangle$
  - $t_k$: Hora atual
  - $\delta(k)$: Cronogramas conjuntos atuais
  - $\sigma(k)$: Status da patrulha
  - $\omega_k$: O novo incidente (local, tempo, dura√ß√£o)

- **A√ß√£o ($x_k$):** tupla de decis√£o $\langle x_k^i, x_k^t, \delta^x(k) \rangle$.
  - $x_k^i$: quem despachar
  - $x_k^t$: quando
  - $\delta^x(k)$: novo cronograma unificado

---

### O MDP: Exemplo de Cronograma Conjunto ($\delta(k)$)

<style>
img[alt~="center"] {
  position: absolute;
  top: 55%;  /* Ponto central vertical (55% para dar espa√ßo ao t√≠tulo) */
  left: 50%; /* Ponto central horizontal */
  transform: translate(-50%, -50%); /* Puxa a imagem de volta pelo seu pr√≥prio centro */
  
  /* Limites para evitar corte */
  max-height: 70%; 
  max-width: 90%;
  
  object-fit: contain; /* Mant√©m a propor√ß√£o sem distorcer */
}
</style>

![center](images/fig7.png)

---

### MDP: Transi√ß√£o e Recompensa

- **Transi√ß√£o ($S_k \rightarrow S_k^x$):**
  - A transi√ß√£o do estado √© determin√≠stica.
  - A A√ß√£o $x_k$ (o novo cronograma $\delta^x(k)$) √© aplicada.
  - O estado muda de "pr√©-decis√£o" ($S_k$) para "p√≥s-decis√£o" ($S_k^x$).

- **Recompensa ($R$): multiplicativa e diferencial**
    $$R(S_k, x_k) = f_r(x_k) \times (f_p(\delta^x(k)) - f_p(\delta(k)))$$
  - **Componentes:**
    - $f_r(x_k) \rightarrow$ Sucesso da Resposta (1.0, 0.5, 0)
    - $f_p(\delta^x(k)) - f_p(\delta(k)) \rightarrow$ Mudan√ßa na Qualidade da Patrulha

---

### MDP: A Fun√ß√£o Objetivo

- O objetivo do agente √© encontrar a a√ß√£o $x_k^*$ que resolve a Equa√ß√£o de Otimiza√ß√£o:
  $$x_k^* = \text{argmax}_{x_k} \{ R(S_k, x_k) + \gamma\hat{V}(S_k^x) \}$$

- **A Premissa do Aprendizado (RL):**
  - Para resolver esta equa√ß√£o, o agente precisa de uma forma de *calcular* o segundo termo.
  - O **Valor Futuro ($\hat{V}(S_k^x)$)** √© desconhecido.
  - O agente precisa *aprender* a estimar o "valor" de um estado p√≥s-decis√£o a partir da experi√™ncia.

---

### Solu√ß√£o: O Avaliador de A√ß√µes (VFA)

- O $\hat{V}$ (o "Avaliador") √© uma **Rede de Fun√ß√£o Valor (VFN)** que estima o Valor Futuro $\hat{V}(S_k^x)$.

- √â treinado **Offline** (antes da execu√ß√£o), usando **Aprendizado por Diferen√ßa Temporal (TD Learning)**.

- O treinamento utiliza **Dados Hist√≥ricos** (o princ√≠pio do *Experience Replay*).

- **Prop√≥sito:** Produzir um "c√©rebro" treinado que sabe dar a "nota" (o $\hat{V}$) para qualquer novo cronograma.

---

### O Avaliador ($\hat{V}$): Diagrama da Rede

<style>
img[alt~="center"] {
  position: absolute;
  top: 55%;
  left: 50%;
  transform: translate(-50%, -50%);
  
  max-height: 80%; 
  max-width: 95%;
  
  object-fit: contain;
}
</style>

![center](images/fig11.png)

---

### Solu√ß√£o: O Gerador de A√ß√µes - Ejection Chains (EC)

- **Processo (Online):**
  - 1. A EC *insere* o incidente $\omega_k$ no cronograma de uma Viatura A (criando um "defeito").
  - 2. A EC *repara* o defeito (ex: "ejetando" uma tarefa de A para B), criando uma "rea√ß√£o em cadeia".

- **Conex√£o Chave (EC + VÃÇ):**
  - A $\hat{V}$ √© consultada *durante* a cadeia de reparo para guiar a heur√≠stica a escolher o melhor reparo ("greedy") a cada passo.

---

### Arquitetura: Treinamento (Offline) e Execu√ß√£o (Online)

<style>
img[alt~="center"] {
  position: absolute;
  top: 55%;
  left: 50%;
  transform: translate(-50%, -50%);
  
  max-height: 80%; 
  max-width: 95%;
  
  object-fit: contain;
}
</style>

![center](images/fig5.png)

---

### Pontos para Discuss√£o e Considera√ß√µes

<style scoped>
section {
  font-size: 30px; /* O padr√£o √© ~28px. Ajuste 24px para caber (tente 22px, 20px, etc.) */
}
</style>

- **1. Modelo "Agn√≥stico" √† Gravidade:**
  - A fun√ß√£o de recompensa ($R$) trata todos os incidentes da mesma forma (1.0, 0.5, 0 pelo *tempo* de resposta).
  - Ela n√£o usa uma tabela de prioridade (ex: "Risco √† Vida" vs. "Furto").

- **2. Risco da Otimiza√ß√£o "Pura":**
  - O sistema pode (corretamente) decidir *postergar* um incidente grave, se ele tiver um "custo" muito alto para a patrulha (baixa recompensa futura esperada).

- **3. Sugest√£o de Melhoria:**
  - Incorporar um **fator de prioridade ($p$)** (baseado na gravidade) diretamente na fun√ß√£o de recompensa, como um multiplicador.

---

### Resumo das Abordagens

<style scoped>
section {
  font-size: 30px; /* O padr√£o √© ~28px. Ajuste 24px para caber (tente 22px, 20px, etc.) */
}
</style>

- **Problema:** Patrulha Integrada (Proativa) & Despacho (Reativo).

- **H√≠brida/Heur√≠stica (Sim√µes J√∫nior & Borenstein, 2025):**
  - Poderosa, mas depende de heur√≠sticas de PO (como ACO) para *construir* a pol√≠tica.

- **MARL Descentralizado (Repasky et al., 2024):**
  - "Bottom-up". Trata viaturas como agentes, mas *decomp√µe* o problema (Patrulha vs. Despacho).

- **MORL Centralizado (Joe, Lau, & Pan, 2022):**
  - Abordagem hol√≠stica. Usa RL ($\hat{V}$) para *guiar* uma heur√≠stica (EC) e resolver o problema unificado.

---

# Obrigado! üôå  
Perguntas?
