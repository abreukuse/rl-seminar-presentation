---
marp: true
theme: gaia
paginate: true
---

# RL Multi-Objetivo Aplicado ao Problema Integrado de Patrulha e Despacho

Moacir Almeida Sim√µes J√∫nior
Tobias de Abreu Kuse



---

### O Problema Central: Um Conflito de Objetivos

- Opera√ß√µes policiais t√™m dois objetivos conflitantes e simult√¢neos.  
- **1. Patrulhamento Proativo:** Maximizar a *presen√ßa* em √°reas de alto risco ("hotspots") para dissuadir o crime.  
- **2. Despacho Reativo:** Minimizar o *tempo de resposta* a novas chamadas de emerg√™ncia din√¢micas.  
- **O Desafio:** Cada vez que uma unidade √© despachada *reativamente*, ela compromete o plano de patrulha *proativo*.  
- Este √© um problema **bi-objetivo**.

---

### Um Panorama das Solu√ß√µes

- Problema complexo: mistura previs√£o espa√ßo-temporal, roteamento e eventos estoc√°sticos em tempo real.  
- Tr√™s filosofias de solu√ß√£o principais:  
  1. Modelos **H√≠bridos Heur√≠sticos** (Simula√ß√£o + Meta-heur√≠sticas)  
  2. Modelos **MARL Descentralizados** (RL Multiagente)  
  3. Modelos **MORL Centralizados** (RL Multi-Objetivo)

---

### Abordagem 1: A H√≠brida-Heur√≠stica  
*(Sim√µes J√∫nior & Borenstein, 2025)*

- **Conceito:** Modelar o sistema com simula√ß√£o de eventos discretos.  
- **M√©todo:**  
  - **Prever:** Usar ML (ex: XGBoost) para prever *hotspots din√¢micos*.  
  - **Patrulhar:** Usar **ACO** para rotas de patrulha ideais.  
  - **Testar:** Simular interrup√ß√µes por chamadas aleat√≥rias.  
- **Conclus√£o:** Abordagem de *"otimizar e depois simular"*.

---

### Abordagem 2: O Modelo Multiagente  
*(Repasky et al., 2024)*

- **Conceito:** Cada viatura = um agente de RL.  
- **M√©todo:**  
  - Sistema **heterog√™neo** com $N+1$ agentes.  
  - **$N$ Patrulheiros:** aprendem pol√≠ticas pr√≥prias (DQN compartilhada).  
  - **1 Despachante:** aprende pol√≠tica central (MIP + VFA).  
- **Conclus√£o:** Abordagem **descentralizada**, com agentes aprendendo individualmente.

---

### Abordagem 3: RL Multi-Objetivo (MORL)

- RL padr√£o ‚Üí recompensa escalar $R$.  
- MORL ‚Üí vetor de recompensas $\vec{R} = \langle R_{\text{resposta}}, R_{\text{presen√ßa}} \rangle$.  
- **Desafio:** "√ìtimo" √© subjetivo (quanto vale piorar patrulha pra melhorar resposta?).  
- **Solu√ß√£o comum:** Soma ponderada  
  $$R_{total} = w_1 R_{resposta} + w_2 R_{presen√ßa}$$  
- **Problema:** Altamente sens√≠vel aos pesos ‚Äî arbitr√°rios e dif√≠ceis de justificar.

---

### Vis√£o Geral e Formula√ß√£o

- Para contornar as dificuldades da abordagem de soma ponderada, o artigo de **Joe, Lau, & Pan (2022)** apresenta um modelo MORL centralizado que n√£o utiliza essa t√©cnica.
- **T√≠tulo:** *Reinforcement Learning Approach to Solve Dynamic Bi-objective Police Patrol Dispatching and Rescheduling Problem*  
- **Formula√ß√£o:** MDP Centralizado de Agente √önico.  
- **Agente de RL:** Planejador central / despachante.  
- **Atores:** Unidades de patrulha (recursos controlados).  
- N√£o √© MARL ‚Äî unidades n√£o aprendem.

---

### O MDP: Estado e A√ß√£o

- **Estado ($S_k$):** snapshot do sistema no incidente $\omega_k$.  
  - Hora atual ($t_k$), cronogramas ($\delta(k)$), status de patrulha ($\sigma(k)$).  
- **A√ß√£o ($x_k$):** tupla de decis√£o $\langle x_k^i, x_k^t, \delta^x(k) \rangle$.  
  - $x_k^i$: quem despachar  
  - $x_k^t$: quando  
  - $\delta^x(k)$: novo cronograma conjunto  
- **Insight:** espa√ßo de a√ß√£o enorme ‚Äî decis√£o global sobre todas as unidades.

---

### A Solu√ß√£o: H√≠brido de VFA + Heur√≠stica

- **Divis√£o de trabalho:**  
  - **Offline (C√©rebro):**  
    - Rede de Fun√ß√£o Valor $\hat{V}$ treinada offline.  
    - Aprende o valor futuro esperado de cada estado.  
  - **Online (Gerador de A√ß√£o):**  
    - Heur√≠stica de Reagendamento (Ejection Chains) gera a√ß√µes vi√°veis.  
    - $\hat{V}$ avalia cada a√ß√£o e escolhe:
      $$x_k^* = \text{argmax}_{x_k} \{ R_{imediata} + \gamma \hat{V}(S_k^x) \}$$

---

### Inova√ß√£o Bi-Objetivo (Sem Soma Ponderada)

- Recompensa **multiplicativa e diferencial**:
  $$R(S_k, x_k) = f_r(x_k) \times f_p(\delta^x(k)) - f_p(\delta(k))$$
- **Componentes:**
  - $f_r(x_k)$ ‚Üí sucesso da resposta (1.0, 0.5, 0)  
  - $f_p(\delta^x(k)) - f_p(\delta(k))$ ‚Üí mudan√ßa na presen√ßa da patrulha  
- **Intelig√™ncia:**  
  - A recompensa pondera *resposta* √ó *impacto na patrulha*.  
  - Uma falha que destr√≥i o plano de patrulha ‚Üí fortemente penalizada.

---

### Resultados Chave

- **Joint Learning > Two-Stage**  
  - O h√≠brido ($\hat{V}$ + heur√≠stica) supera o m√©todo "Two-Stage".  
- **Computacionalmente Vi√°vel:**  
  - Despacho + reagendamento em tempo quase real (<10s).



---

### Resumo das Abordagens

- **Problema:** Patrulha Conjunta (Proativa) & Despacho (Reativo).  
- **H√≠brida/Heur√≠stica:** poderosa, mas depende de heur√≠sticas (ACO).  
- **MARL:** baseado em agentes, mas decomp√µe o problema.  
- **MORL Centralizado:** abordagem hol√≠stica (VFA + heur√≠stica inteligente).

---

### Principais Conclus√µes

- Problema real e ideal para RL avan√ßado.  
- Arquitetura de Joe, Lau & Pan:  
  - **Gerador-Heur√≠stico + Avaliador-VFA** ‚Üí √≥timo para espa√ßos de a√ß√£o complexos.  
- Recompensa multiplicativa ‚Üí resolve bi-objetivo sem pesos arbitr√°rios.

---

# Obrigado! üôå  
Perguntas?
